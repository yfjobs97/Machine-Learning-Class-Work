{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNOA8Zs2L74_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "233Z6TfZUtu2"
   },
   "source": [
    "## You can put your name here\n",
    "\n",
    "* Name:\n",
    "* Net ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBpseTfaP-J5"
   },
   "source": [
    "### Note: This template is used to save your time. If there are some typos in this file, please **refer to the original PDF file of problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxribM91QFVK"
   },
   "source": [
    "Problem Set 4\n",
    "\n",
    "CS 6375\n",
    "\n",
    "Due: 4/21/2022 by 11:59pm\n",
    "\n",
    "Note: all answers should be accompanied by explanations and relevant code for full credit. All\n",
    "code (Python or MATLAB only) should be turned in with your answers to the following questions. Python is preferred.  \n",
    "Late homeworks will not be accepted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTfiN9n3QNfJ"
   },
   "source": [
    "The following code help you download the data files needed for the problems below. After running code, these files will appear in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwTzxz2RQOgo",
    "outputId": "ec815793-8d85-46a3-ea67-06b54b3fa1ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'leaf.data' already there; not retrieving.\n",
      "\n",
      "File 'prostate_GE.data' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grab Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Downloading data files for you.\n",
    "# Remember to run this cell!!\n",
    "\n",
    "!wget -nc https://personal.utdallas.edu/~yangxiao.lu/22SP_CS6375/leaf.data\n",
    "!wget -nc https://personal.utdallas.edu/~yangxiao.lu/22SP_CS6375/prostate_GE.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbSrZeZdMF2I"
   },
   "source": [
    "#$\\color{blue}{\\text{Problem 1:}} \\text{ PCA and Feature Selection} \\color{red}{\\text{(50 pts)}}$#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEFjmi7vMLGW"
   },
   "source": [
    "In this problem, we will explore ways that we can use PCA for the problem of generating or selecting \"good\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYg-nUuWML7U"
   },
   "source": [
    "##1.1 SVMs and PCA (25 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrR4dh1pMQx1"
   },
   "source": [
    "Consider the prostate GE data set (attached to this problem set). Each row corresponds to a single\n",
    "data point and the final column in each row is the class label (1 or 2 here). You should use the\n",
    "first 80 data points for training and the remaining 10% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN4d448PMj15"
   },
   "source": [
    "* Perform PCA on the training data to reduce the dimensionality of the data set (ignoring the\n",
    "class labels for the moment). What are the top six eigenvalues of the data covariance matrix?\n",
    "Looking at the eigenvalues, how would you recommend picking k for this data set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "3xSEXPBlMsC0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributions of each eigen value:  [5.70393485e-01 4.53756540e-02 4.11866780e-02 2.06098695e-02\n",
      " 1.64962389e-02 1.61281384e-02 1.32959658e-02 1.16461022e-02\n",
      " 1.10719621e-02 1.03953876e-02 9.31934774e-03 8.78355201e-03\n",
      " 8.16483103e-03 7.70609411e-03 7.45004888e-03 7.15805429e-03\n",
      " 6.92469672e-03 6.73605211e-03 6.47834449e-03 6.29482718e-03\n",
      " 6.19111620e-03 5.86850209e-03 5.74144359e-03 5.61275323e-03\n",
      " 5.38048638e-03 5.20629269e-03 4.96566507e-03 4.89605081e-03\n",
      " 4.71313577e-03 4.69398191e-03 4.54133824e-03 4.33303641e-03\n",
      " 4.07871200e-03 3.89868367e-03 3.81493159e-03 3.72359111e-03\n",
      " 3.61028952e-03 3.57723339e-03 3.43525956e-03 3.31371713e-03\n",
      " 3.26609401e-03 3.15966104e-03 3.06905152e-03 2.97378529e-03\n",
      " 2.89043898e-03 2.80645551e-03 2.76688678e-03 2.70464152e-03\n",
      " 2.60200913e-03 2.50907787e-03 2.47191191e-03 2.44241306e-03\n",
      " 2.33491010e-03 2.27630158e-03 2.23719427e-03 2.08708704e-03\n",
      " 2.01460970e-03 2.00108819e-03 1.92522312e-03 1.87384167e-03\n",
      " 1.80386576e-03 1.76851632e-03 1.73001141e-03 1.64681898e-03\n",
      " 1.63125373e-03 1.58188513e-03 1.55248705e-03 1.52894530e-03\n",
      " 1.48916357e-03 1.44737371e-03 1.38896041e-03 1.32856434e-03\n",
      " 1.24983542e-03 1.23096641e-03 1.15930360e-03 1.13677259e-03\n",
      " 9.78946676e-04 9.46856607e-04 7.75240157e-04 2.42069788e-31] \n",
      " Contribution of first 25 eigen values:  0.8604096316602614\n",
      "Top 6 eigen values:  [275.40003748  21.90848449  19.88594359   9.9509531    7.96479084\n",
      "   7.78706283] \n",
      " Contribution of first 6 eigen values:  0.7101900639439733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputData =  pd.read_csv('prostate_GE.data', sep=\",\", header=None)\n",
    "trainData = inputData[:80]\n",
    "testData = inputData[80:]\n",
    "\n",
    "pca = PCA()\n",
    "X = trainData.drop(columns=[5966])\n",
    "\n",
    "#print(Y)\n",
    "XTrans = pca.fit_transform(X)\n",
    "#print(XTrans)\n",
    "eig_vals = pca.explained_variance_\n",
    "contrib_func = np.vectorize(lambda x: x / np.sum(eig_vals))\n",
    "var_contrib = contrib_func(eig_vals)\n",
    "print(\"Contributions of each eigen value: \",var_contrib, \"\\n Contribution of first 25 eigen values: \", np.sum(var_contrib[:25]))\n",
    "\n",
    "print(\"Top 6 eigen values: \", eig_vals[:6], \"\\n Contribution of first 6 eigen values: \", np.sum(var_contrib[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the contribution, The top 6 eigenvalues contributes to 71% of the information. k=25 will retain 86% of the original information, so k=25 is recomended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqOMjNqcMrN9"
   },
   "source": [
    "* For each k ∈ {1, 2, 3, 4, 5, 6}, project the training data into the best k dimensional subspace\n",
    "(with respect to the Frobenius norm) and use the SVM with slack formulation to learn a\n",
    "classifier for each c ∈ {1, 10, 100, 1000}. As data is limited, this process should be done with\n",
    "10-fold cross-validation. Report the average error of the learned classifier on the held out\n",
    "validation data for each k and c pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "id": "G6fUF_KzMVVm"
   },
   "outputs": [],
   "source": [
    "k = [1,2,3,4,5,6]\n",
    "c = [1,10,100,1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k =  1 and C = 1 : Hit =  11 in a total of  22\n",
      "With k =  1 and C = 10 : Hit =  11 in a total of  22\n",
      "With k =  1 and C = 100 : Hit =  11 in a total of  22\n",
      "With k =  1 and C = 1000 : Hit =  11 in a total of  22\n",
      "Average Accuracy of all c is: 0.5\n",
      "With k =  2 and C = 1 : Hit =  9 in a total of  22\n",
      "With k =  2 and C = 10 : Hit =  10 in a total of  22\n",
      "With k =  2 and C = 100 : Hit =  10 in a total of  22\n",
      "With k =  2 and C = 1000 : Hit =  10 in a total of  22\n",
      "Average Accuracy of all c is: 0.45454545454545453\n",
      "With k =  3 and C = 1 : Hit =  14 in a total of  22\n",
      "With k =  3 and C = 10 : Hit =  16 in a total of  22\n",
      "With k =  3 and C = 100 : Hit =  14 in a total of  22\n",
      "With k =  3 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  4 and C = 1 : Hit =  14 in a total of  22\n",
      "With k =  4 and C = 10 : Hit =  15 in a total of  22\n",
      "With k =  4 and C = 100 : Hit =  15 in a total of  22\n",
      "With k =  4 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  5 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  5 and C = 10 : Hit =  15 in a total of  22\n",
      "With k =  5 and C = 100 : Hit =  14 in a total of  22\n",
      "With k =  5 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  6 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  6 and C = 10 : Hit =  15 in a total of  22\n",
      "With k =  6 and C = 100 : Hit =  13 in a total of  22\n",
      "With k =  6 and C = 1000 : Hit =  12 in a total of  22\n",
      "Average Accuracy of all c is: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for kVal in k:\n",
    "    acc=[]\n",
    "    pca = PCA(n_components=kVal)\n",
    "    XTrans = pca.fit_transform(X)\n",
    "    XTest_Trans = pca.fit_transform(XTest)\n",
    "    Y = trainData[5966].values\n",
    "    Y.reshape(-1,1)\n",
    "    XTest = testData.drop(columns=[5966])\n",
    "    YTest = testData[5966].values\n",
    "\n",
    "    for cSelection in c:\n",
    "        acc=[]\n",
    "        clf = make_pipeline(StandardScaler(), SVC(C=cSelection, kernel='poly', gamma='auto'))\n",
    "        clf.fit(XTrans, Y)\n",
    "        YPred = clf.predict(XTest_Trans)\n",
    "\n",
    "        res = sum([int(YTest[i] == YPred[i]) for i in range(YTest.shape[0])])\n",
    "        acc.append(res/YTest.shape[0])\n",
    "        print(\"With k = \", kVal, \"and C =\", cSelection, ': Hit = ', res, 'in a total of ',YTest.shape[0])\n",
    "    print('Average Accuracy of all c is:', np.array(acc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ssArp0tM6FG"
   },
   "source": [
    "* What is the performance you achieve on the test set via the proper hyperparameter selection\n",
    "procedure above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZEqUzaSMGZ_"
   },
   "source": [
    "With k = 3 and c =10, The performance reaches the highest (15 correct out of 22). \n",
    "k = 3 is the best k value due to its eliminatio of more values but reaching a relatively good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afQ56zPVM7s3"
   },
   "source": [
    "* Now suppose that we don’t do proper hyperparameter selection. What is the best performance\n",
    "that you can achieve on the test set if you tune the hyperparameters using the test set instead\n",
    "of the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "id": "h7chSW1nM_I4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k =  7 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  7 and C = 10 : Hit =  13 in a total of  22\n",
      "With k =  7 and C = 100 : Hit =  13 in a total of  22\n",
      "With k =  7 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  8 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  8 and C = 10 : Hit =  12 in a total of  22\n",
      "With k =  8 and C = 100 : Hit =  13 in a total of  22\n",
      "With k =  8 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  9 and C = 1 : Hit =  14 in a total of  22\n",
      "With k =  9 and C = 10 : Hit =  11 in a total of  22\n",
      "With k =  9 and C = 100 : Hit =  12 in a total of  22\n",
      "With k =  9 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  10 and C = 1 : Hit =  14 in a total of  22\n",
      "With k =  10 and C = 10 : Hit =  14 in a total of  22\n",
      "With k =  10 and C = 100 : Hit =  15 in a total of  22\n",
      "With k =  10 and C = 1000 : Hit =  15 in a total of  22\n",
      "Average Accuracy of all c is: 0.6818181818181818\n",
      "With k =  11 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  11 and C = 10 : Hit =  14 in a total of  22\n",
      "With k =  11 and C = 100 : Hit =  14 in a total of  22\n",
      "With k =  11 and C = 1000 : Hit =  14 in a total of  22\n",
      "Average Accuracy of all c is: 0.6363636363636364\n",
      "With k =  12 and C = 1 : Hit =  13 in a total of  22\n",
      "With k =  12 and C = 10 : Hit =  16 in a total of  22\n",
      "With k =  12 and C = 100 : Hit =  14 in a total of  22\n",
      "With k =  12 and C = 1000 : Hit =  14 in a total of  22\n",
      "Average Accuracy of all c is: 0.6363636363636364\n",
      "With k =  13 and C = 1 : Hit =  12 in a total of  22\n",
      "With k =  13 and C = 10 : Hit =  15 in a total of  22\n",
      "With k =  13 and C = 100 : Hit =  14 in a total of  22\n",
      "With k =  13 and C = 1000 : Hit =  13 in a total of  22\n",
      "Average Accuracy of all c is: 0.5909090909090909\n",
      "With k =  14 and C = 1 : Hit =  11 in a total of  22\n",
      "With k =  14 and C = 10 : Hit =  16 in a total of  22\n",
      "With k =  14 and C = 100 : Hit =  15 in a total of  22\n",
      "With k =  14 and C = 1000 : Hit =  15 in a total of  22\n",
      "Average Accuracy of all c is: 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "for kVal in range(7,15):\n",
    "    acc=[]\n",
    "    pca = PCA(n_components=kVal)\n",
    "    XTrans = pca.fit_transform(X)\n",
    "    XTest_Trans = pca.fit_transform(XTest)\n",
    "    Y = trainData[5966].values\n",
    "    Y.reshape(-1,1)\n",
    "    XTest = testData.drop(columns=[5966])\n",
    "    YTest = testData[5966].values\n",
    "\n",
    "    for cSelection in c:\n",
    "        acc=[]\n",
    "        clf = make_pipeline(StandardScaler(), SVC(C=cSelection, kernel='poly', gamma='auto'))\n",
    "        clf.fit(XTrans, Y)\n",
    "        YPred = clf.predict(XTest_Trans)\n",
    "\n",
    "        res = sum([int(YTest[i] == YPred[i]) for i in range(YTest.shape[0])])\n",
    "        acc.append(res/YTest.shape[0])\n",
    "        print(\"With k = \", kVal, \"and C =\", cSelection, ': Hit = ', res, 'in a total of ',YTest.shape[0])\n",
    "    print('Average Accuracy of all c is:', np.array(acc).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, from the testing set, k = 10 produces the best accuracy at 68%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12TpfJz_NG7C"
   },
   "source": [
    "## 1.2 PCA for Feature Selection (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG-asfMHNLde"
   },
   "source": [
    "If we performed PCA directly on the training data as we did in the first part of this question, we\n",
    "would generate new features that are linear combinations of our original features. If instead, we\n",
    "wanted to find a subset of our current features that were good for classification, we could still use\n",
    "PCA, but we would need to be more clever about it. The primary idea in this approach is to select features from the data that are good at explaining as much of the variance as possible. To do this,\n",
    "we can use the results of PCA as a guide. Implement the following algorithm for a given k and s:\n",
    "1.   Compute the top $k$ eigenvalues and eigenvector of the covariance matrix corresponding to the data matrix omitting the labels (recall that the columns of the data matrix are the input data points). Denote the top $k$ eigenvectors as $v^{(1)}, ... , v^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5-rTTn_Na7x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Tz8FcQWNPyp"
   },
   "source": [
    "2.   Define $\\pi_j = \\frac{1}{k}\\sum_{i=1}^k v_j^{(i)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhscElqFNAB_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFN8RbEsNf99"
   },
   "source": [
    "3.   Sample $s$ features independently from the probability distribution defined by $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqUwvn8eNlFn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUe4bMP4Nllm"
   },
   "source": [
    "*  Why does $\\pi$ define a probability distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSD1BzKONr1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcn5iI_yNsKg"
   },
   "source": [
    "*  Again, using the prostate_GE data set and same procedure as above, for each $k\\in\\{1,10,20,40,80,160\\}$ with $s=\\lfloor k\\log k\\rfloor$, report the average test error of the SVM with slack classifier over $20$ experiments. For each experiment use only the $s$ selected features (note that there may be some duplicates, so only include each feature once). Use the same hyperparameter search for $c$ as in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCRVnn8hNyN7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKPQaKzNOMHJ"
   },
   "source": [
    "* Does this provide a reasonable alternative to the SVM with slack formulation without feature\n",
    "selection on this data set? What are the pros and cons of this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P__DNAuRONHr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JMJTbi6OTGW"
   },
   "source": [
    "#$\\color{blue}{\\text{Problem 2:}} \\text{ Working with k-means} \\color{red}{\\text{(50 pts)}}$#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftEKxSWHObwB"
   },
   "source": [
    "For this problem, you will use the leaf.data file provided with this problem set. This data set\n",
    "was generated from the UCI Leaf Data Set (follow the link for information about the format of the\n",
    "data). The class labels are still in the data set and should be used for evaluation only (i.e., don’t\n",
    "use them in the clustering procedure), but the specimen number has been removed. You should\n",
    "preprocess the data so that the non-label attributes have mean zero and variance one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcKKLATuOdlL"
   },
   "source": [
    "## 2.1 Train a k-means classifier for each $k\\in\\{10, 15, 20, 25, 30\\}$ starting from twenty different random initializations (sample uniformly from $[−3, 3]$ for each attribute) for each $k$. Report the\n",
    "mean and variance of the value of the $k$-means objective obtained for each $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATA_AMOUNT=340\n",
    "RV=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "ebNkxePLOX2C"
   },
   "outputs": [],
   "source": [
    "col_names = ['y','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14']\n",
    "trainData =  pd.read_csv('leaf.data', sep=\",\" , names = col_names)\n",
    "#TrainDataNew contains preprocessed Data\n",
    "trainDataNew = trainData.drop(columns=['y'])\n",
    "#Store the preAdjusted Mean and Standard Deviation\n",
    "trainMean = trainDataNew.mean()\n",
    "trainSTD = trainDataNew.std()\n",
    "#print(\"trainMean is: \\n\", trainMean)\n",
    "#print(\"trainSTD is \\n\", trainSTD)\n",
    "trainDataNew = (trainDataNew-trainMean)/trainSTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  10\n",
      "         x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0 -0.265226 -0.304245 -0.489363  0.378863  0.392753  0.339310 -0.228871   \n",
      "1 -1.406347 -0.518779  0.859338 -3.186378 -2.894113 -1.727637  2.439976   \n",
      "2  0.861924  0.025556  0.514247  0.315729  0.319266 -0.085595 -0.265616   \n",
      "3 -1.384623 -0.515278  0.671503 -1.509767 -1.623472 -1.457132  1.111967   \n",
      "4  1.303199  2.587141  1.978144 -1.515707 -0.220531 -1.674218  3.297187   \n",
      "5 -0.884856 -0.502723  0.580235 -1.601683 -2.330870 -1.301411  1.996235   \n",
      "6  0.191413 -0.270899 -0.441537  0.522929  0.437678  0.752471 -0.577058   \n",
      "7  0.179777 -0.271835  0.264699 -0.515079 -0.480975 -0.651743  0.793029   \n",
      "8 -0.680756 -0.430448 -1.037842  0.446010  0.443593  0.782074 -0.538156   \n",
      "9  1.307830  2.751569  1.914578  0.159189  0.239481 -1.479951 -0.137185   \n",
      "\n",
      "         x8        x9       x10       x11       x12       x13       x14  \n",
      "0 -0.311394  1.824712  1.615862  1.783753  1.542576  1.641771  1.484010  \n",
      "1  2.538360 -0.591451 -0.575098 -0.570887 -0.503793 -0.536043 -0.512388  \n",
      "2 -0.323975  0.355822  0.564280  0.420233  0.460383 -0.056465  0.422202  \n",
      "3  0.638746  1.209139  1.067332  1.022753  0.814793  0.750350  1.480426  \n",
      "4  4.359561 -1.100766 -1.200414 -0.978109 -0.821441 -0.826813 -1.385953  \n",
      "5  1.799929 -0.864609 -0.962383 -0.865606 -0.795524 -0.691241 -0.757851  \n",
      "6 -0.442488 -0.818305 -0.864824 -0.788402 -0.696317 -0.606937 -0.820097  \n",
      "7  0.419141 -0.452921 -0.298986 -0.403758 -0.309325 -0.494960 -0.384980  \n",
      "8 -0.432375  0.175183  0.257164  0.105908  0.059771  0.325626  0.302062  \n",
      "9 -0.269429 -0.841620 -0.864603 -0.803263 -0.721827 -0.682781 -0.883344  \n",
      "K =  10 center's Mean is: \n",
      " x1    -0.077767\n",
      "x2     0.255006\n",
      "x3     0.481400\n",
      "x4    -0.650589\n",
      "x5    -0.571719\n",
      "x6    -0.650383\n",
      "x7     0.789151\n",
      "x8     0.797608\n",
      "x9    -0.110482\n",
      "x10   -0.126167\n",
      "x11   -0.107738\n",
      "x12   -0.097070\n",
      "x13   -0.117749\n",
      "x14   -0.105591\n",
      "dtype: float64\n",
      "K =  10 center's Variance is \n",
      " x1     1.041320\n",
      "x2     1.647479\n",
      "x3     0.959257\n",
      "x4     1.563193\n",
      "x5     1.573080\n",
      "x6     1.032221\n",
      "x7     1.912447\n",
      "x8     2.618661\n",
      "x9     0.968773\n",
      "x10    0.917593\n",
      "x11    0.862361\n",
      "x12    0.647526\n",
      "x13    0.639704\n",
      "x14    0.990084\n",
      "dtype: float64\n",
      "K =  15\n",
      "          x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0   1.333659  3.681659  2.206376 -2.042127  0.000583 -1.931184  4.190168   \n",
      "1   1.337019  5.594030  2.189844 -0.364806 -0.192586 -1.979269  0.250795   \n",
      "2   0.846156  0.013288  0.497921  0.252477  0.289958 -0.132461 -0.198709   \n",
      "3  -1.336180 -0.516390  0.833729 -2.918403 -2.863531 -1.669261  2.344244   \n",
      "4   1.322881  2.548473  2.110100 -1.871549 -0.487394 -1.762220  3.798074   \n",
      "5  -0.725280 -0.436948 -1.090043  0.454451  0.448969  0.815330 -0.567600   \n",
      "6  -0.688222 -0.423341 -0.851373  0.314684  0.308831  0.524564  0.172619   \n",
      "7   0.315326 -0.257796  0.295720 -0.595261 -0.312030 -0.666437  0.442127   \n",
      "8   0.643937 -0.090818  0.165354  0.550162  0.452982  0.345481 -0.538415   \n",
      "9  -1.384623 -0.515278  0.671503 -1.509767 -1.623472 -1.457132  1.111967   \n",
      "10  0.218887 -0.268005 -0.425823  0.537571  0.432589  0.771717 -0.571607   \n",
      "11  1.327826  3.107410  2.135792 -1.312141 -0.190036 -1.830578  2.608182   \n",
      "12 -1.021406 -0.451758 -0.991662  0.277843  0.394377  0.251750 -0.253897   \n",
      "13 -0.204335 -0.291996  0.360959 -0.652899 -1.151579 -0.826609  1.944498   \n",
      "14  1.304181  2.396261  1.880170  0.224689  0.293489 -1.417537 -0.185683   \n",
      "\n",
      "          x8        x9       x10       x11       x12       x13       x14  \n",
      "0   6.427570 -1.143652 -1.415954 -1.095247 -0.993568 -0.792517 -1.305591  \n",
      "1  -0.057150 -1.132544 -1.311274 -1.048801 -0.910511 -0.840032 -1.267302  \n",
      "2  -0.299940  0.131586  0.383791  0.222713  0.321597 -0.258032  0.211057  \n",
      "3   2.378391 -0.650892 -0.658761 -0.637543 -0.571739 -0.566733 -0.562369  \n",
      "4   5.417460 -1.168048 -1.310506 -1.041722 -0.895294 -0.849164 -1.522551  \n",
      "5  -0.443936  0.044391  0.145127 -0.001016 -0.009285  0.123264  0.185684  \n",
      "6  -0.147058  2.160485  2.337123  2.870236  3.325184  0.711027  1.255794  \n",
      "7   0.077650 -0.385879 -0.245908 -0.378110 -0.307306 -0.454009 -0.197392  \n",
      "8  -0.430438  1.224492  1.158127  1.128156  0.859406  0.985454  1.140322  \n",
      "9   0.638746  1.209139  1.067332  1.022753  0.814793  0.750350  1.480426  \n",
      "10 -0.434909 -0.864585 -0.937320 -0.834346 -0.740650 -0.621330 -0.883225  \n",
      "11  2.869317 -1.164378 -1.360477 -1.073190 -0.949607 -0.837830 -1.434615  \n",
      "12 -0.327653  1.689055  1.112129  1.072767  0.348089  2.873543  1.677090  \n",
      "13  1.764917 -0.761134 -0.658137 -0.644925 -0.493758 -0.692889 -0.870271  \n",
      "14 -0.295964 -0.805254 -0.808769 -0.772571 -0.698241 -0.663124 -0.835349  \n",
      "K =  15 center's Mean is: \n",
      " x1     0.219322\n",
      "x2     0.939253\n",
      "x3     0.665905\n",
      "x4    -0.577005\n",
      "x5    -0.279923\n",
      "x6    -0.730923\n",
      "x7     0.969784\n",
      "x8     1.142467\n",
      "x9    -0.107815\n",
      "x10   -0.166898\n",
      "x11   -0.080723\n",
      "x12   -0.060059\n",
      "x13   -0.075468\n",
      "x14   -0.195219\n",
      "dtype: float64\n",
      "K =  15 center's Variance is \n",
      " x1     1.080285\n",
      "x2     3.917572\n",
      "x3     1.448430\n",
      "x4     1.228534\n",
      "x5     0.886385\n",
      "x6     1.133710\n",
      "x7     2.611192\n",
      "x8     4.956004\n",
      "x9     1.298962\n",
      "x10    1.334493\n",
      "x11    1.318181\n",
      "x12    1.275044\n",
      "x13    1.063090\n",
      "x14    1.261886\n",
      "dtype: float64\n",
      "K =  20\n",
      "          x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0  -2.422359 -0.527063  1.285897 -1.536366 -2.218171 -1.739534  1.098081   \n",
      "1  -0.622257 -0.487414 -0.041706 -0.333529 -0.936391 -0.532067  1.391925   \n",
      "2  -1.299587 -0.513903  0.776218 -2.825467 -2.774808 -1.635206  2.334947   \n",
      "3  -0.688222 -0.423341 -0.851373  0.314684  0.308831  0.524564  0.172619   \n",
      "4   0.785480 -0.031404  0.339644  0.545636  0.457013  0.244297 -0.534577   \n",
      "5   1.320937  2.520463  2.117411 -1.895406 -0.655152 -1.776540  3.919267   \n",
      "6  -1.494444 -0.509509 -0.010138 -1.453192 -0.563624 -1.071183  1.098827   \n",
      "7   1.337019  5.594030  2.189844 -0.364806 -0.192586 -1.979269  0.250795   \n",
      "8   0.163642 -0.275771 -0.471363  0.535093  0.442639  0.781607 -0.587154   \n",
      "9   1.133909  0.394988  1.084781 -0.818941 -0.205291 -0.905770  2.650437   \n",
      "10 -0.648211 -0.428153 -1.034760  0.455163  0.446694  0.797582 -0.552387   \n",
      "11  1.304181  2.396261  1.880170  0.224689  0.293489 -1.417537 -0.185683   \n",
      "12 -1.196990 -0.509022  1.516541 -2.090418 -3.555578 -1.915363  1.535241   \n",
      "13 -0.733127 -0.482396 -0.015640 -1.090109 -0.574539 -1.100816  0.903732   \n",
      "14  1.333659  3.681659  2.206376 -2.042127  0.000583 -1.931184  4.190168   \n",
      "15  0.841414 -0.024417  0.503497  0.067839  0.170120 -0.243290 -0.095245   \n",
      "16  1.324826  2.576483  2.102788 -1.847691 -0.319636 -1.747901  3.676881   \n",
      "17 -1.043979 -0.453608 -0.983003  0.269916  0.389442  0.214688 -0.252804   \n",
      "18 -1.632363 -0.525640  1.220017 -1.500210 -1.821723 -1.517359  0.992482   \n",
      "19  1.327826  3.107410  2.135792 -1.312141 -0.190036 -1.830578  2.608182   \n",
      "\n",
      "          x8        x9       x10       x11       x12       x13       x14  \n",
      "0   0.608341  1.980782  1.444576  1.517008  0.770956  2.517678  1.776632  \n",
      "1   0.997693 -0.892677 -0.826079 -0.773726 -0.614693 -0.752364 -1.048570  \n",
      "2   2.362279 -0.722387 -0.739052 -0.719833 -0.658166 -0.595911 -0.625350  \n",
      "3  -0.147058  2.160485  2.337123  2.870236  3.325184  0.711027  1.255794  \n",
      "4  -0.432917  1.278754  1.214964  1.199431  0.942598  0.949136  1.188186  \n",
      "5   5.718382 -1.058764 -1.071389 -0.940402 -0.803668 -0.816854 -1.346849  \n",
      "6   0.639765  0.185539  0.177888 -0.001416 -0.037814 -0.114294  0.895985  \n",
      "7  -0.057150 -1.132544 -1.311274 -1.048801 -0.910511 -0.840032 -1.267302  \n",
      "8  -0.444535 -0.845267 -0.909404 -0.818012 -0.727901 -0.616178 -0.851264  \n",
      "9   2.911158 -0.379435  0.011187 -0.127188  0.126817 -0.549099 -0.643930  \n",
      "10 -0.438091  0.158151  0.246748  0.096022  0.056577  0.284449  0.282192  \n",
      "11 -0.295964 -0.805254 -0.808769 -0.772571 -0.698241 -0.663124 -0.835349  \n",
      "12  1.195937  1.274610  1.449744  1.511141  1.549470  0.389618  0.998147  \n",
      "13  0.426885  1.241688  0.990576  0.900315  0.685104  0.635850  1.888211  \n",
      "14  6.427570 -1.143652 -1.415954 -1.095247 -0.993568 -0.792517 -1.305591  \n",
      "15 -0.242223  0.007278  0.220047  0.061814  0.139627 -0.286080  0.129823  \n",
      "16  5.116538 -1.277333 -1.549622 -1.143041 -0.986919 -0.881475 -1.698253  \n",
      "17 -0.324542  1.765447  1.172592  1.142241  0.396382  2.896327  1.731803  \n",
      "18  0.497485  0.431091  0.851629  0.734859  0.953828 -0.208878  0.199229  \n",
      "19  2.869317 -1.164378 -1.360477 -1.073190 -0.949607 -0.837830 -1.434615  \n",
      "K =  20 center's Mean is: \n",
      " x1    -0.045432\n",
      "x2     0.753983\n",
      "x3     0.797550\n",
      "x4    -0.834869\n",
      "x5    -0.574936\n",
      "x6    -0.939043\n",
      "x7     1.230787\n",
      "x8     1.369444\n",
      "x9     0.053107\n",
      "x10    0.006253\n",
      "x11    0.075982\n",
      "x12    0.078273\n",
      "x13    0.021472\n",
      "x14   -0.035554\n",
      "dtype: float64\n",
      "K =  20 center's Variance is \n",
      " x1     1.574864\n",
      "x2     3.393482\n",
      "x3     1.290512\n",
      "x4     1.113465\n",
      "x5     1.321442\n",
      "x6     0.968601\n",
      "x7     2.371868\n",
      "x8     4.708494\n",
      "x9     1.380162\n",
      "x10    1.378043\n",
      "x11    1.308201\n",
      "x12    1.187409\n",
      "x13    1.180115\n",
      "x14    1.474430\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  25\n",
      "          x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0  -1.547095 -0.514177  1.450643 -1.932118 -3.173462 -1.865126  1.410338   \n",
      "1   1.320937  2.520463  2.117411 -1.895406 -0.655152 -1.776540  3.919267   \n",
      "2   1.333659  3.681659  2.206376 -2.042127  0.000583 -1.931184  4.190168   \n",
      "3  -0.937584 -0.513129 -0.090961 -1.185686 -0.543313 -1.122873  1.004847   \n",
      "4   1.338651  6.386117  2.221971 -0.463696 -0.319636 -2.045913  0.925560   \n",
      "5   0.513311 -0.191381 -0.130526  0.513677  0.449031  0.636196 -0.569464   \n",
      "6   0.810505 -0.020173  0.366577  0.543768  0.457540  0.219757 -0.524113   \n",
      "7  -1.299587 -0.513903  0.776218 -2.825467 -2.774808 -1.635206  2.334947   \n",
      "8   1.327826  3.107410  2.135792 -1.312141 -0.190036 -1.830578  2.608182   \n",
      "9  -1.212534 -0.481951 -0.747920  0.098241  0.290291  0.223951  0.362681   \n",
      "10 -0.746447 -0.451854 -1.031112  0.337516  0.430906  0.594873 -0.475001   \n",
      "11  1.334283  4.761672  2.148311 -0.109629 -0.014687 -1.864547 -0.147625   \n",
      "12 -1.721388 -0.532084 -1.286566  0.305936  0.198850  0.284034  0.318222   \n",
      "13  1.324826  2.576483  2.102788 -1.847691 -0.319636 -1.747901  3.676881   \n",
      "14  0.035437 -0.145688  0.636710 -0.969911 -0.957065 -0.967545  2.183388   \n",
      "15 -0.589649 -0.479406 -0.369097  0.099757 -0.521706 -0.186127  0.946305   \n",
      "16 -0.487616 -0.417044 -1.104081  0.595580  0.464878  1.122756 -0.687924   \n",
      "17  0.850130  0.026281  0.420440  0.461393  0.419094  0.107386 -0.390735   \n",
      "18  1.303040  2.331358  1.870726  0.221334  0.296943 -1.405180 -0.182030   \n",
      "19 -1.672192 -0.525051  0.545311 -1.560337 -1.179705 -1.328761  1.095064   \n",
      "20 -0.660085 -0.421371 -0.986163  0.407897  0.400877  0.456619 -0.394675   \n",
      "21  0.305437 -0.335788 -0.364664  0.264516  0.315730  0.548670  0.059436   \n",
      "22 -1.392334 -0.482852 -1.206381  0.358935  0.318628  0.538264  0.248832   \n",
      "23 -1.052286 -0.455926 -0.991271  0.251458  0.406216  0.305371 -0.241535   \n",
      "24  0.678386 -0.182132  0.503163 -0.615041 -0.310024 -0.757189  0.369404   \n",
      "\n",
      "          x8        x9       x10       x11       x12       x13       x14  \n",
      "0   1.028052  1.476373  1.448268  1.512818  1.327037  0.997635  1.220572  \n",
      "1   5.718382 -1.058764 -1.071389 -0.940402 -0.803668 -0.816854 -1.346849  \n",
      "2   6.427570 -1.143652 -1.415954 -1.095247 -0.993568 -0.792517 -1.305591  \n",
      "3   0.512519  1.319179  0.959067  0.860522  0.560178  0.785169  2.096029  \n",
      "4   0.430269 -1.210342 -1.474091 -1.116883 -0.977386 -0.867614 -1.406061  \n",
      "5  -0.442272 -0.934446 -1.061389 -0.901876 -0.810192 -0.657393 -0.992031  \n",
      "6  -0.425543  1.257988  1.207697  1.190208  0.953818  0.886268  1.179926  \n",
      "7   2.362279 -0.722387 -0.739052 -0.719833 -0.658166 -0.595911 -0.625350  \n",
      "8   2.869317 -1.164378 -1.360477 -1.073190 -0.949607 -0.837830 -1.434615  \n",
      "9  -0.042539  2.399384  1.210676  1.179859  0.044556  5.907142  2.380886  \n",
      "10 -0.418883  0.258133  0.477663  0.318464  0.375623  0.065153  0.259318  \n",
      "11 -0.326176 -1.111232 -1.248778 -1.023989 -0.889370 -0.827439 -1.284509  \n",
      "12 -0.072847  3.873877  3.013427  4.028996  3.380595  1.999993  2.560931  \n",
      "13  5.116538 -1.277333 -1.549622 -1.143041 -0.986919 -0.881475 -1.698253  \n",
      "14  2.128983 -0.711003 -0.675381 -0.621232 -0.483757 -0.637213 -0.748960  \n",
      "15  0.506649 -0.875808 -0.698421 -0.705351 -0.516772 -0.758897 -1.159510  \n",
      "16 -0.475866 -0.520111 -0.509554 -0.572270 -0.546420 -0.326618 -0.331705  \n",
      "17 -0.380478  0.035254  0.249657  0.082147  0.152044 -0.164225  0.128353  \n",
      "18 -0.292449 -0.790422 -0.787993 -0.760837 -0.688862 -0.655876 -0.808291  \n",
      "19  0.628800  0.367326  0.487454  0.331734  0.361768 -0.056224  0.761609  \n",
      "20 -0.364951  2.069688  1.338449  1.355986  0.477866  3.282762  1.961508  \n",
      "21 -0.200847  2.031340  2.402001  2.967918  3.701864  0.726853  0.965603  \n",
      "22 -0.111556  2.026410  2.184899  2.620972  2.994399  0.513323  1.318081  \n",
      "23 -0.337446  0.971227  0.749315  0.637813  0.226931  1.755908  1.019144  \n",
      "24 -0.001948  0.001162  0.162439  0.018026  0.063157 -0.312158  0.210654  \n",
      "K =  25 center's Mean is: \n",
      " x1    -0.033695\n",
      "x2     0.749101\n",
      "x3     0.447748\n",
      "x4    -0.491970\n",
      "x5    -0.260386\n",
      "x6    -0.617072\n",
      "x7     0.881617\n",
      "x8     0.953422\n",
      "x9     0.262699\n",
      "x10    0.131956\n",
      "x11    0.257252\n",
      "x12    0.212606\n",
      "x13    0.309279\n",
      "x14    0.116836\n",
      "dtype: float64\n",
      "K =  25 center's Variance is \n",
      " x1     1.304037\n",
      "x2     3.919927\n",
      "x3     1.606280\n",
      "x4     1.074259\n",
      "x5     0.893489\n",
      "x6     1.134778\n",
      "x7     2.172189\n",
      "x8     4.141337\n",
      "x9     2.078435\n",
      "x10    1.778019\n",
      "x11    2.021572\n",
      "x12    1.839707\n",
      "x13    2.508391\n",
      "x14    1.814867\n",
      "dtype: float64\n",
      "K =  30\n",
      "          x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0   1.338651  6.386117  2.221971 -0.463696 -0.319636 -2.045913  0.925560   \n",
      "1   1.320937  2.520463  2.117411 -1.895406 -0.655152 -1.776540  3.919267   \n",
      "2  -0.589649 -0.479406 -0.369097  0.099757 -0.521706 -0.186127  0.946305   \n",
      "3   0.305437 -0.335788 -0.364664  0.264516  0.315730  0.548670  0.059436   \n",
      "4   1.336203  5.197987  2.173781 -0.315361 -0.129061 -1.945948 -0.086587   \n",
      "5  -1.472981 -0.496694 -0.137028 -1.251089 -0.411129 -0.920563  0.957135   \n",
      "6  -1.437562 -0.521255  0.774208 -3.211218 -2.622315 -1.702030  2.405482   \n",
      "7  -1.246649 -0.464765 -1.335275  0.439980  0.407230  0.292907 -0.599576   \n",
      "8   1.133909  0.394988  1.084781 -0.818941 -0.205291 -0.905770  2.650437   \n",
      "9   1.327826  3.107410  2.135792 -1.312141 -0.190036 -1.830578  2.608182   \n",
      "10 -1.721388 -0.532084 -1.286566  0.305936  0.198850  0.284034  0.318222   \n",
      "11  0.225493 -0.273822 -0.457980  0.555043  0.454934  0.827263 -0.608827   \n",
      "12  1.256818  1.357794  1.600137  0.510978  0.341148 -1.015367 -0.352475   \n",
      "13  1.323142  3.060376  2.026460  0.006693  0.272882 -1.654853 -0.077053   \n",
      "14  0.743900 -0.109464  0.496663 -0.248390 -0.015932 -0.561199  0.272240   \n",
      "15 -0.809579 -0.494818  0.556956 -1.237179 -2.395739 -1.192572  2.017265   \n",
      "16  0.677082 -0.083167  0.151336  0.591888  0.459395  0.419967 -0.619841   \n",
      "17 -1.613041 -0.516381  1.356807 -1.664202 -2.637556 -1.751100  1.174566   \n",
      "18  1.324826  2.576483  2.102788 -1.847691 -0.319636 -1.747901  3.676881   \n",
      "19 -1.212534 -0.481951 -0.747920  0.098241  0.290291  0.223951  0.362681   \n",
      "20  1.333659  3.681659  2.206376 -2.042127  0.000583 -1.931184  4.190168   \n",
      "21 -1.064150 -0.468433 -1.111483  0.313961  0.315730  0.382258  0.292947   \n",
      "22  0.808317 -0.163686  0.555633 -0.653818 -0.343223 -0.731083  0.309498   \n",
      "23 -0.937584 -0.513129 -0.090961 -1.185686 -0.543313 -1.122873  1.004847   \n",
      "24 -1.251482 -0.475025 -0.998653  0.154359  0.390715  0.061580 -0.153762   \n",
      "25  0.850671 -0.007922  0.402979  0.460531  0.416673  0.155479 -0.433271   \n",
      "26 -1.333047 -0.490377 -1.315900  0.434975  0.375276  0.798344 -0.129342   \n",
      "27  0.424512 -0.159749 -0.071099  0.545048  0.448676  0.453640 -0.447905   \n",
      "28 -0.721523 -0.441190 -1.099081  0.452387  0.452981  0.811251 -0.575009   \n",
      "29 -0.883002 -0.509345  1.385318 -2.666303 -4.414109 -1.900003  2.215828   \n",
      "\n",
      "          x8        x9       x10       x11       x12       x13       x14  \n",
      "0   0.430269 -1.210342 -1.474091 -1.116883 -0.977386 -0.867614 -1.406061  \n",
      "1   5.718382 -1.058764 -1.071389 -0.940402 -0.803668 -0.816854 -1.346849  \n",
      "2   0.506649 -0.875808 -0.698421 -0.705351 -0.516772 -0.758897 -1.159510  \n",
      "3  -0.200847  2.031340  2.402001  2.967918  3.701864  0.726853  0.965603  \n",
      "4  -0.300859 -1.093645 -1.229865 -1.014761 -0.877074 -0.826241 -1.197923  \n",
      "5   0.507185  0.239307  0.286720  0.113676  0.089749 -0.071211  0.784145  \n",
      "6   2.481937 -0.737158 -0.771945 -0.744597 -0.687404 -0.578964 -0.633495  \n",
      "7  -0.466366  2.519131  1.532763  1.621612  0.542000  3.865916  2.321327  \n",
      "8   2.911158 -0.379435  0.011187 -0.127188  0.126817 -0.549099 -0.643930  \n",
      "9   2.869317 -1.164378 -1.360477 -1.073190 -0.949607 -0.837830 -1.434615  \n",
      "10 -0.072847  3.873877  3.013427  4.028996  3.380595  1.999993  2.560931  \n",
      "11 -0.452816 -0.882295 -0.969808 -0.852891 -0.758824 -0.631436 -0.901320  \n",
      "12 -0.396510 -0.576998 -0.514605 -0.572071 -0.539134 -0.545151 -0.528940  \n",
      "13 -0.226459 -0.958329 -1.004089 -0.898117 -0.790568 -0.737912 -1.063120  \n",
      "14 -0.086394  0.463940  0.744080  0.609390  0.745969 -0.237401  0.573483  \n",
      "15  1.836212 -0.875764 -0.964793 -0.866108 -0.787962 -0.724452 -0.775439  \n",
      "16 -0.452492  1.060611  1.074589  1.018976  0.825959  0.672104  1.010473  \n",
      "17  0.704076  1.293035  1.327602  1.358358  1.221132  0.837711  1.032387  \n",
      "18  5.116538 -1.277333 -1.549622 -1.143041 -0.986919 -0.881475 -1.698253  \n",
      "19 -0.042539  2.399384  1.210676  1.179859  0.044556  5.907142  2.380886  \n",
      "20  6.427570 -1.143652 -1.415954 -1.095247 -0.993568 -0.792517 -1.305591  \n",
      "21 -0.084977  2.618487  2.505260  3.137807  3.262540  0.885940  1.768995  \n",
      "22 -0.023349 -0.337495 -0.248396 -0.379284 -0.361969 -0.362199 -0.105408  \n",
      "23  0.512519  1.319179  0.959067  0.860522  0.560178  0.785169  2.096029  \n",
      "24 -0.304947  1.156525  0.870398  0.773075  0.301403  1.839751  1.228219  \n",
      "25 -0.398849 -0.053042  0.145569 -0.015888  0.050898 -0.232345  0.058444  \n",
      "26 -0.271651  1.176228  1.635807  1.791573  2.396890 -0.001852  0.690202  \n",
      "27 -0.398281  1.698261  1.281592  1.286489  0.675437  2.235929  1.600849  \n",
      "28 -0.449853 -0.046196  0.051002 -0.095930 -0.109837  0.057890  0.097885  \n",
      "29  2.152530  0.412948  0.719253  0.612686  0.738703 -0.210176  0.333109  \n",
      "K =  30 center's Mean is: \n",
      " x1    -0.018760\n",
      "x2     0.675494\n",
      "x3     0.465456\n",
      "x4    -0.519299\n",
      "x5    -0.352758\n",
      "x6    -0.655409\n",
      "x7     0.874110\n",
      "x8     0.918144\n",
      "x9     0.319721\n",
      "x10    0.216585\n",
      "x11    0.324000\n",
      "x12    0.284133\n",
      "x13    0.305026\n",
      "x14    0.176750\n",
      "dtype: float64\n",
      "K =  30 center's Variance is \n",
      " x1     1.324887\n",
      "x2     3.632172\n",
      "x3     1.532167\n",
      "x4     1.148165\n",
      "x5     1.340635\n",
      "x6     1.034791\n",
      "x7     2.057573\n",
      "x8     3.755486\n",
      "x9     2.002023\n",
      "x10    1.689825\n",
      "x11    1.939927\n",
      "x12    1.785750\n",
      "x13    2.413702\n",
      "x14    1.713918\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "kVal=[10,15,20,25,30]\n",
    "\n",
    "\n",
    "for k in kVal:\n",
    "    centersSelected = []\n",
    "    for iterations in range(k):\n",
    "        centersFetSelected = []\n",
    "        RV=np.random.uniform(low=-3,high=3,size=20)\n",
    "        RV=((RV%N_DATA_AMOUNT))\n",
    "        for idx in range(20):\n",
    "            RV[idx] = int(RV[idx])\n",
    "\n",
    "        rowsSelected = trainDataNew.iloc[RV,0:]\n",
    "        for idx in range(14):\n",
    "            centersFetSelected.append(rowsSelected.mean()[idx])\n",
    "        centersSelected.append(centersFetSelected)\n",
    "        \n",
    "    centersSelected = np.array(centersSelected)\n",
    "    kmeans = KMeans(n_clusters = k, init=centersSelected, random_state=1, n_init=1)\n",
    "    kmeans.fit(trainDataNew)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    #print(centers)\n",
    "    centers = pd.DataFrame(data=centers[:,:], columns=['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14'])\n",
    "    centersMean = centers.mean()\n",
    "    centersSTD = centers.std()\n",
    "    print(\"K = \", k)\n",
    "    print(centers)\n",
    "    print(\"K = \", k, \"center's Mean is: \\n\", centersMean)\n",
    "    print(\"K = \", k, \"center's Variance is \\n\", centersSTD**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UHDwrYMOz-D"
   },
   "source": [
    "## 2.2 Random initializations can easily get stuck in suboptimal clusterings. An improvement of the\n",
    "k-means algorithm, known as k-means++, instead chooses an initialization as follows:\n",
    "\n",
    "  (a) Choose a data point uniformly at random to be the first center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3pYJV1sO-lY"
   },
   "source": [
    "(b) Repeat the following until k centers have been selected:  \n",
    "i. For each data point x compute the distance between x and the nearest cluster center\n",
    "in the current set of centers. Denote this distance as dx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYGInQKqPGP5"
   },
   "source": [
    "ii. Sample a training data point at random from the distribution $p$ such that $p(x) ∝ d^2_x $\n",
    "Add the sampled point to the current set of centers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcpuTz4KPhqH"
   },
   "source": [
    "Repeat the first experiment using this initialization to pick the initial cluster centers for\n",
    "k-means. Does this procedure result in an improvement? Explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "id": "S8Uzbj01PMMz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36, 277]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RowNumber = []\n",
    "RowNumber.append(np.random.uniform(0, N_DATA_AMOUNT))\n",
    "RowNumber[0] = int(FirstCenterRowNumber)\n",
    "print(RowNumber[0])\n",
    "\n",
    "for k in kVal:\n",
    "    RowSelected = trainDataNew.iloc[RowNumber[0],0:]\n",
    "    distances=[]\n",
    "    for idx in range(N_DATA_AMOUNT):\n",
    "        distances.append(np.linalg.norm(firstRowSelected - trainDataNew.iloc[idx,0:]))\n",
    "    #Make largest distance point the next point will fit d^2x\n",
    "    #distances= np.array(distances)\n",
    "    RowNumber.append(np.where(distances == np.amax(distances))[0][0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6375 HW4 Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
